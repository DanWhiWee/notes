# 注意力机制

- 目前大多数注意力模型附着在Encoder-Decoder框架下，但注意力模型本身可以看作一种通用的思想，本身不依赖于特定的框架

#### Soft Attention model

- 在英语-德语翻译系统中加入Attention机制后，Source和Target两个句子每个单词对应的注意力分配概率分布

![](dependencies\v2-cfa6cd2cd4a855d165b16ad0f51cee90_r.png)

- 如何理解attention模型的物理含义？一般来说，在自然语言处理应用中，可以把AttentIon模型看作是输出Target句子中某个单词与Source句中每个单词的对其模型

- Attention计算机制可抽象为3个阶段：

  1. 计算query与key的相关性或者相似性，如求点积、cosine相似性等，结果为权重得分
     $$
     点积：Similarity(Query,Key_i)=Query \cdot Key_i \newline
     Cosine相似性：Similarity(Query,Key_i)=\frac{Query \cdot Key_i \newline}{\|Query\| \cdot \|Key_i\|} \newline
     MLP网络：Similarity(Query,Key_i)=MLP(Query,Key_i)
     $$

  2. 引入类似softmax的机制对第一阶段的得分进行归一化处理，这种方式可以将原始分值整理成权重之和为1的概率分布，同时也可以突出重要元素的权重。
     $$
     a_i=Softmax(Sim_i)=\frac{e^{Sim_i}}{\sum_{i=1}^{L_x}e^{Sim_i}},其中L_x=\|Source\|代表Source的长度
     $$

  3. 步骤2中的$a_i$即为$Value_i$对应的权重系数，最后将$Value$加权求和即可得到$Query$相对于$Source$的Attention数值

- 计算机制如下：

  ![](dependencies\v2-07c4c02a9bdecb23d9664992f142eaa5_r.png)

#### Self Attention model

- 如前文所述，soft attention是在Source与Target之间进行运算，本质是目标语单词与源语单词之间的一种对其机制，而self attention则是source或者target内部元素间的attention机制，计算过程与soft attention类似。
- Self Attention抽取的关键信息：
  - 捕获句子中单词间的句法特征。如make something more diffcult句法，make在more以及diffcult上的注意力高于句中其它单词。
  - 捕获语义特征。如句子中的代词实际指代的对象
  - 其它中长距离的相互依赖特征